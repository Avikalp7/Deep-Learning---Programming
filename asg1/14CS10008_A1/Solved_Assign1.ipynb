{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Assignment 1\n",
    "\n",
    "- Do not submit your answers as images where text or an equation is expected. You might be evaluated with a zero.\n",
    "- Use mathlatex (latex notations) to type math equations\n",
    "- If at all you feel you need to add some diagram or illustration, use relative path to add them as image and make sure you include them in the zipped archive that you will be submitting in the moodle\n",
    "- Name your notebook and the zip as < rollno >_A1.zip. For example if you are roll number 13CS60R12 then submit the zip as 13CS60R12_A1.zip\n",
    "\n",
    "- The marks for the individual questions will be decided later\n",
    "- Double click on the cells where it is written \"Ans. Write your answer here.\". Markdown syntax needs to be followed while writing the answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. \n",
    "We are given a dataset $(X,Y)$.  Which among the following classifiers will contain sufficient information that allows the calculation of joint probability of the features and the label in the dataset? Justify your answer for each of the classifer.\n",
    "If $X = (X_1,X_2,X_3,X_4)$ then you need to calculate $P(X_1,X_2,X_3,X_4,Y)$.\n",
    "\n",
    " - Linear Regression\n",
    " - Logisitc Regression\n",
    " - Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Each classifier's potential use for this task is discussed:\n",
    "\n",
    "- **Linear Regression**: A linear regression model is not suited to perform the task of joint probability calculation. It is used when the dependent variable Y is continuous. It does NOT contain sufficient information.\n",
    "  \n",
    "  Based on learned weigths, a linear regression model predicts the score associated with the unseen instance. Therefore, this model suffers from two major drawbacks for the task at hand:\n",
    "    - Not useful in classifying instances into labels.\n",
    "    - Can't give probability predictions for different classes. Only a predicted value based on learned weights.\n",
    "    \n",
    "    \n",
    "- **Logistic Regression**: This model is very well suited to gives us a joint probablity distibution, and contains sufficient information for this probabilistic classification task. For more than 2 labels, one vs all logistic regression can be applied. \n",
    "\n",
    "Unlike Linear Regression that regresses for a continuous $Y$, Logistic Regression regresses for the probobability of a categorical outcome.\n",
    "\n",
    "  Specifically, logisitic regression uses same generalised linear model as linear regression but models probability for binary classification as $$P(Y_i=1) = \\frac{1}{(1+e^{(-w_i^tx + b)})}$$ where $Y_i$ is the $i^{th}$ label, $b$ is the bias term and $w_i$ is the weight vector learned for $i^{th}$ label using one vs all.\n",
    "\n",
    "\n",
    "\n",
    "- **Gaussian Naive Bayes**: This is used to apply naive bayes when the input $X$ is continuous. The assumption is made that X follows a normal gaussian distribution and that the componenets $x_1, x_2, ....$ are conditionally independent given $Y$ . \n",
    "\n",
    "    So, if the $x_1, x_2, ....$ in $X$ are not binary/discrete, and can be assumed to be normally disributed, then Gaussian Naive Bayes can easily provide us with the joint probability distribution.\n",
    "   \n",
    "   $$P(X = (x_1, x_2, x_3, x_4),Y) = P(X|Y)*P(Y)$$, where $P(X|Y) = \\prod P(x_i|Y)$ by conditional independence condition. $P(x_i|Y)$ is calculated by assuming gaussian distribution on variable $x_i$ for each given label in $Y$.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2a. \n",
    "For two discrete-valued distributions $P(X), Q(X)$, K-L Divergence is defined as\n",
    " \n",
    "$$ KL(P||Q) = \\sum_x P(x)log\\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "where P(x) > 0. \n",
    "\n",
    "Prove the following $$ \\forall P,Q~~ KL(P||Q) \\geq 0 $$ and\n",
    "$$ KL(P||Q) ~ = ~ 0 ~ iff ~ P ~ = ~ Q$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans.** We use here the property of natural logarithm that: $ln(x) <= x - 1$\n",
    "    \n",
    "$$=> -KL(P||Q) <= \\sum_x P(x)(\\frac{Q(x)}{P(x)} - 1)$$\n",
    "\n",
    "$$=> -KL(P||Q) <= \\sum_x Q(x) - 1 <= 0$$   (as Q(x) <= 1 for any x)\n",
    "\n",
    "$$=> KL(P||Q) >= 0 $$\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "For the second part, we'll break the proof into two parts.\n",
    "1. If P = Q, then KL(P||Q) = 0 is trivially true as $log\\frac{P(x)}{Q(x)}$ is always 0 \n",
    "\n",
    "2. If KL(P||Q) = 0, then P = Q.\n",
    "    \n",
    "    From above proof, for this to be true, we'll have $ln(x) = x - 1$ for all x.\n",
    "    \n",
    "    => $\\frac{Q(x)}{P(x)} = 1$ for all x \n",
    "    \n",
    "    => P = Q\n",
    "    \n",
    "    Since P = Q => KL(P||Q) = 0 and KL(P||Q) = 0 => P = Q, we have: $$ KL(P||Q) ~ = ~ 0 ~ iff ~ P ~ = ~ Q$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## Q2b.\n",
    "The KL-Divergence between two conditional distributions $P(X|Y),Q(X|Y)$ is\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(x|y)}{Q(x|y)} \\bigg)$$\n",
    "\n",
    "\n",
    "Prove the following chain rule for KL Divergence:\n",
    "$$ KL(P(X|Y)||Q(X|Y) = KL(P(X)|Q(X)) + KL(P(Y|X)||Q(Y|X)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ans.**\n",
    "$$ KL(P(X|Y)||Q(X|Y)) ~ = ~ \\sum_y P(y) \\bigg( \\sum_x P(x|y)log\\frac{P(x|y)}{Q(x|y)} \\bigg)$$ \n",
    "\n",
    "$$ = \\sum_{x,y}P(y) P(x|y)log\\frac{P(x|y)}{Q(x|y)}  = \\sum_{x,y}P(x,y)log\\frac{P(x|y)}{Q(x|y)}$$\n",
    "\n",
    "$$ = \\sum_{x,y}P(x,y)log\\frac{P(x,y)Q(y)}{Q(x,y)P(y)} = \\sum_{x,y}P(x,y)log\\frac{P(y|x)P(x)Q(y)}{Q(y|x)Q(x)P(y)}$$\n",
    "\n",
    "There is no way forward from here!\n",
    "\n",
    "After going through web, I found that there was a slight mistake in the question, and we have $ KL(P(X,Y)||Q(X,Y))$ on the LHS instead of $ KL(P(X|Y)||Q(X|Y))$\n",
    "\n",
    "Using this, we have,\n",
    "\n",
    "$$ KL(P(X,Y)||Q(X,Y)) ~ = \\sum_{x,y} P(x,y)log\\frac{P(x,y)}{Q(x,y)}$$\n",
    "\n",
    "$$ \\sum_{x,y} P(x,y)log\\frac{P(x) P(y|x)}{Q(x) Q(y|x)} = \\sum_{x,y} P(x,y)log\\frac{P(x)}{Q(x)} + \\sum_{x,y} P(x,y)log\\frac{P(y|x)}{Q(y|x)}$$\n",
    "\n",
    "$$ = \\sum_{x} P(x,y)log\\frac{P(x)}{Q(x)} + \\sum_{x,y} P(x) P(y|x) log\\frac{P(y|x)}{Q(y|x)} = KL(P(X)||Q(X)) + \\sum_{x} P(x) \\sum_{y}P(y|x) log\\frac{P(y|x)}{Q(y|x)}$$\n",
    "\n",
    "$$ = KL(P(X)||Q(X)) + KL(P(Y|X) || Q(Y|X)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. \n",
    "What is the role of the activation function in a neural network? What would happen if you just used the identify function as an activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. Activation Functions are **decision making functions** applied to every node in neural networks that help to achieve **non-linear decision boundaries** (when they are themselves non-linear) - achieving **non-linear combinations of weight inputs**. Other classifiers such as logistic regressor, perceptron, linear regressor remain generalised linear models (GLM's) as they work with linear combination of weight inputs. The application of sigmoid function in these doesn't help in achieving inter-relations between weights.  \n",
    "\n",
    "If we just used identity function as the activation, then the said neural network simply **imitates a linear regressor**, only with extra computational overhead. The reason for this is that, since a node simply 'decides' its output as the computed function itself, it is not making a categroical decision and is simply trying to provide the optimal value of continuous $Y$ given the loss function and input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. \n",
    "Assume a friend of yours recently got diogonised for a rare disease and it is given that the testing methods for this disease are correct 99 percent of the time. You did some googling and found that the chances of the disease to occur randomly in the general population is only one of every 10,000 people. What are the chances that your friend actually have the disease?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. \n",
    "\n",
    "Let $P(d)$ denote probability that a person has the disease.\n",
    "\n",
    "Let $P$(+ve) denote positve diagnosis. We have to find $P$(d | +ve)\n",
    "\n",
    "$P$(d | +ve) = $\\frac{P(+ve | d)*P(d)}{P(+ve)}$\n",
    "\n",
    "where $P$(+ve | d) = 0.99 (given), $P$(d) = 0.0001 (given)\n",
    "\n",
    "Now, $P$(+ve) = $P$(+ve | d) + $P$(+ve | ~d) = 0.99 \\* 0.0001 + 0.01 * 0.9999 = 0.010098\n",
    "\n",
    "So $P$(d | +ve) = $\\frac{0.99 * 0.0001}{0.010098}$ = 0.0098 ~ **1%**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "\n",
    "## Q5. \n",
    "How exactly is the training of structured perceptron different from that of a perceptron? Explain how we can solve argmax problem for sequences in the context of a structured perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. \n",
    "The structured perceptron is used for predictions in generative models through a discriminative algorithm. It is closely related to a log linear model and uses Viterbi approximation to arrive at its update rule.\n",
    "\n",
    "Binary perceptron helps to find prediction on a single scalar output $Y$, while structured perceptron aims at learning weights for predicting a class y' $\\epsilon$ Y for producing a feature vector matching the one generated through $f(x,y)$ (described below). \n",
    "\n",
    "For this, we need a feature vector extractor function $f(x,y)$. Now,\n",
    "\n",
    "* A simple perceptron finds the predicted $y^*$ given weight and input values as follows:\n",
    "    \n",
    "    $$ y^* = \\theta ^T x $$\n",
    "\n",
    "* A structured perceptron find its predicted class $y^*$ given weight and input values as follows:\n",
    "\n",
    "    $$ y^* = argmax_y (\\theta ^T f(x,y)) $$\n",
    "\n",
    "\n",
    "* A simple perceptron seeks to update its weight based on the output scalar $y^*$ and the changes required in weights (based on gradient) to make this value closer to the real value $y$. \n",
    "\n",
    "    This update is captured as: $$ w_i = w_i + \\alpha (y - y^*) $$  \n",
    "\n",
    "\n",
    "* The structured perceptron on the other hand bases its update on the difference in the feature vector predictions, with the goal to learn weights that produce same class prediction as the actual through the above mentioned argmax prediction.\n",
    "\n",
    "    $$ \\theta = \\theta + \\alpha (f(x,y) - f(x,y^*) $$\n",
    "\n",
    "\n",
    "The argmax calculation required for structured perceptron is non trivial and requires dynamic programming based Viterbi Algorithm to produce the sequence $y^*$ that satisfies argmax condition. This is also possible only for limited $f$ and space $Y$ of labels y.\n",
    "\n",
    "Given that our function $f(x,y)$ is decomposable over vecctor representation of $Y$ such that no feature depends on elements of y that are more than k positions away, complexity is given by $T * M^k$ where $T$ is the sequence length and $M$ is the number of possible labels.\n",
    "\n",
    "For example, for Markov models we have k=2 (output depends only on previous unit), and hence complexity is $T * M^2$ \n",
    "\n",
    "The generalised Viterbi algorithm is too large to mention here and is present [here](https://en.wikipedia.org/wiki/Viterbi_algorithm).\n",
    "\n",
    "It bases itself on dynamic programming paradigm and keeps track of the sequence that produces the max.\n",
    "\n",
    "Suppose we have to find $z^*$ = argmax( p(z|x) ) for a simple markov model task.\n",
    "\n",
    "For this we define $\\mu_k (z_k) = max_{z_{1:k-1}} p(z_{1:k}|x_{1:k})$\n",
    "\n",
    "and formulate this as a recursive relations as\n",
    "\n",
    "$ \\mu_k(z_k) = max_{z_{k-1}}\\bigg((p(x_k|z_k)p(z_k|z_{k-1})\\bigg)\\mu_{k-1}(z_{k-1}) $\n",
    "and\n",
    "$ \\mu_1(z_1) = p(z_1,x_1) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------\n",
    "\n",
    "## Q6. \n",
    "We already know differentiation in the context of univariate real valued functions (input $\\in \\mathbb{R}$, output $\\in \\mathbb{R}$). For e.g $\\frac{d}{dx}(x^2 +x) = 2x + 1$.\n",
    "\n",
    "Now we define differentiation in the context of matrices and vectors. Consider a function $f(\\mathbf{x}) = \\mathbf{y}$, where $\\mathbf{x} = (x_1, x_2, \\dots, x_n)^T \\in \\mathbb{R}^n$ and $\\mathbf{y} = (y_1, y_2, \\dots, y_m)^T \\in \\mathbb{R}^m$ are vectors. We define the derivative of $f(\\mathbf{x})$ wrt $\\mathbf{x}$ as\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x}) = \n",
    "    \\begin{bmatrix}\n",
    "    \\frac{\\partial y_1}{\\partial x_1} & \\frac{\\partial y_1}{\\partial x_2} & \\cdots & \\frac{\\partial y_1}{\\partial x_n}\\\\[2mm]\n",
    "\t\\frac{\\partial y_2}{\\partial x_1} & \\frac{\\partial y_2}{\\partial x_2} & \\cdots & \\frac{\\partial y_2}{\\partial x_n}\\\\\n",
    "\t\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\t\\frac{\\partial y_m}{\\partial x_1} & \\frac{\\partial y_m}{\\partial x_2} & \\cdots & \\frac{\\partial y_m}{\\partial x_n}\\\\\n",
    "\t\\end{bmatrix}\n",
    "\\text{ or } \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_{ij} = \\frac{\\partial y_i}{\\partial x_j}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{x}$ is a scalar (denote by $x$) then we define the derivative as a vector of elementwise derivatives given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial x} f(x)\\right]_i = \\frac{\\partial y_i}{\\partial x}\n",
    "$$\n",
    "Similarly, if $\\mathbf{y}$ is a scalar (denote by $y$) then we define the derivative as,\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_i = \\frac{\\partial y}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "If $\\mathbf{y}$ is a scalar (denote by $y$) and $\\mathbf{x}$ is a matrix (denote by $X$), then we define the derivative as a matrix given by\n",
    "$$\n",
    "    \\left[\\frac{\\partial}{\\partial X} f(X)\\right]_{ij} = \\frac{\\partial y}{\\partial X_{ij}}\n",
    "$$\n",
    "\n",
    "Given that $A, X\\in \\mathbb{R}^{a\\times b}$ and $v, x\\in \\mathbb{R}^{b}$, show the following:\n",
    " - $\\frac{\\partial}{\\partial x} v^T x = \\frac{\\partial}{\\partial x} x^T v = v$\n",
    " - $\\frac{\\partial}{\\partial x} Ax = A$\n",
    " - $\\frac{\\partial}{\\partial x} X^TAx = Ax + A^TX$\n",
    "\n",
    "Using the above results, show the following result (which is actually the solution to least squares linear regression)\n",
    "$$\n",
    "    \\underset{w}{\\arg \\min} \\| Xw-Y\\|_2^2 = (X^TX)^{-1}X^TY\n",
    "$$\n",
    "(Hint: $\\|v\\|_2^2 = v^Tv$. Write the above norm in this form, differentiate and equate to zero.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. \n",
    "\n",
    "1. First, we must see that $\\mathbf{v}^T \\mathbf{x}$ is a scalar (1 x 1). And hence, $\\mathbf{v}^T \\mathbf{x}$ = $\\mathbf{x}^T \\mathbf{v}$\n",
    "\n",
    "   and, $\\mathbf{v}^T \\mathbf{x}$ = $v_1 * x_1 + v_2 * x_2 + .... v_b * x_b$\n",
    "\n",
    "   Now, $$\\left[\\frac{\\partial}{\\partial \\mathbf{x}} f(\\mathbf{x})\\right]_i = \\frac{\\partial y}{\\partial x_i}$$\n",
    "   \n",
    "   => $$\\left[\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{v}^T \\mathbf{x})\\right]_i = \\frac{\\partial(v_1*x_1 + v_2*x_2 + .... v_b*x_b)}{\\partial x_i} = v_i$$\n",
    "   \n",
    "   => $$\\left[\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{v}^T \\mathbf{x})\\right] = \\mathbf{v}$$\n",
    "   \n",
    "   --------------------------------------------\n",
    "   \n",
    "2. $$\\left[\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{A} \\mathbf{x})\\right]_i = \\frac{\\partial (\\mathbf{Ax})_i}{\\partial \\mathbf{x}} = \\frac{\\partial (\\mathbf{a}^{(i)}\\mathbf{x})}{\\partial \\mathbf{x}}$$\n",
    "   where $\\mathbf{a}^{(i)}$ denotes i$^{th}$ row of $\\mathbf{A}$\n",
    "   \n",
    "   Here, $\\mathbf{a}^{(i)} \\mathbf{x} = \\mathbf{a}^{(i)}_1 \\mathbf{x}_1 + \\mathbf{a}^{(i)}_2 \\mathbf{x}_2 + ... +\\mathbf{a}^{(i)}_b \\mathbf{x}_b $\n",
    "   \n",
    "   Hence, from part 1, $$\\frac{\\partial (\\mathbf{a}^{(i)}\\mathbf{x})}{\\partial \\mathbf{x}} = \\mathbf{a}^{(i)}$$\n",
    "   \n",
    "   => $$\\left[\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{A} \\mathbf{x})\\right]_i = \\mathbf{a}^{(i)}$$\n",
    "   \n",
    "   => $$\\left[\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{A} \\mathbf{x})\\right]_{ij} = \\mathbf{a}_{(i,j)}$$\n",
    "   \n",
    "   => $$\\left[\\frac{\\partial}{\\partial \\mathbf{x}} (\\mathbf{A} \\mathbf{x})\\right] = \\mathbf{A}$$\n",
    "   \n",
    "    --------------------------------------------\n",
    "3. $$\n",
    "     \\| Xw-Y\\|_2^2 = (Xw-Y)^T (Xw-Y) = (Y^T - (Xw)^T)(Xw - Y) = Y^TXw - Y^TY - (Xw)^TXw + (Xw)^TY\n",
    "$$\n",
    "\n",
    "    Now,\n",
    "    $$\n",
    "    Y^TXw - Y^TY - (Xw)^TXw + (Xw)^TY = (Xw)^TXw + 2(Xw)^TY - Y^TY\n",
    "    $$\n",
    "    \n",
    "    Doing analysis for derivative of each term,\n",
    "    \n",
    "    Using part 1 result,\n",
    "    $$ \\left[\\frac{\\partial}{\\partial \\mathbf{w}} ((\\mathbf{Xw})^T \\mathbf{Xw})\\right] = ((\\mathbf{Xw})^T)^T \\mathbf{X} + \\mathbf{X}^T\\mathbf{Xw} = 2\\mathbf{X}^T\\mathbf{Xw}$$\n",
    "    \n",
    "    $$ \\left[\\frac{\\partial}{\\partial \\mathbf{w}} (2(\\mathbf{Xw})^T \\mathbf{Y})\\right] = 2\\mathbf{X}^T\\mathbf{Y}$$\n",
    "    \n",
    "    $$ \\left[\\frac{\\partial}{\\partial \\mathbf{w}} (\\mathbf{Y}^T \\mathbf{Y})\\right] = 0$$\n",
    "    \n",
    "    So,\n",
    "    $$ \\left[\\frac{\\partial}{\\partial \\mathbf{w}} (\\| Xw-Y\\|_2^2)\\right] = 2\\mathbf{X}^T\\mathbf{Xw} - 2\\mathbf{X}^T\\mathbf{Y} $$\n",
    "    \n",
    "    Equating derivative to zero for minima,\n",
    "    $$ 2\\mathbf{X}^T\\mathbf{Xw} = 2\\mathbf{X}^T\\mathbf{Y} $$\n",
    "    => $$ \\mathbf{X}^T\\mathbf{Xw} = \\mathbf{X}^T\\mathbf{Y} $$\n",
    "    => $$ \\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{Y} $$ \n",
    "    \n",
    "Thus, we have successfully derived the normal equation for solving weights for linear regression problem.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------\n",
    "## Q7a \n",
    "You are given a Neural Network model which claims to detect between Huskies and Wolves. You are also shown the predictions of the model on 10 held-out images. \n",
    "\n",
    "The results show amongs 10 images (5 each from both the classes), it mis-predicts the 2 cases (1 each from each of the class - 6th and 9th images) of the 10 images.\n",
    "\n",
    "\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "This particular question does not look for the exact answer and rather this question wants to test your thinking and reasoning capacity. So try to come up with multiple possible explanations. <i>The subsequent question will show what exactly was the neural network learning, and hence it is implied that we expect different answers from what is given below. So attempt the next question only after finishing this question</i> \n",
    "\n",
    "![Wolf or Huskies](images/7a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans. \n",
    "\n",
    "#### Subjective Analysis \n",
    "\n",
    "* The model is tested on 10 hold-out images i.e. these images were not used while learning in train set and neither in tuning hyperparameters in cross-validation test. Thus, they represent a valid test set.\n",
    "\n",
    "\n",
    "* There is concern regarding the size of this test-set. Assuming common neural network architectures, such classification would have required at least around ~5k image training set (80% of total labelled images available). In correspondence with this, test set should have contained at least around ~1.2k images to get a good evaluation of the model.\n",
    "\n",
    "\n",
    "* There is not enough information about the representation power of this test set. Ideally, the set should be picked from the distribution from which unseen instances are expected. We have no such information about this abysmally small test set.\n",
    "\n",
    "\n",
    "* The above two points in combination help in removing unwanted bias and variation to a large extent from the test set and give a good estimate on performance on unseen data. In their absence and in presence of small data-set, while variance is not an issue, high bias is certainly a big factor.\n",
    "\n",
    "\n",
    "* While statistical analysis below provides good results, the mentioned factors of test-set size and bias lead to us wanting performance on many more examples, specifically since the images classes seem difficult to classify for a human as well. \n",
    "\n",
    "\n",
    "#### Statistical Analysis\n",
    "\n",
    "* Assuming the data is not largely skewed towards any of the 2 classes, accuracy gives a good measure.\n",
    "\n",
    "    $$ Accuracy = \\frac{Correctly\\_Classified\\_Examples}{Total\\_Examples} = \\frac{8}{10} = 80\\% $$\n",
    "    \n",
    "* Assuming 'husky' to be the positive class ($y$ = 1) and 'wolf' to be negative class ($y$ = 1), we can compute the precision-recall evaluation metrics using the confusion matrix:\n",
    "    \n",
    "|  | Predicted +ve  | Predicted -ve  |\n",
    "|---|---|---|\n",
    "| Actual +ve  | TP = 4  | FN = 1  |\n",
    "| Actual -ve  | FP = 1  | TN = 4  |\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP + FP} = \\frac{4}{5} = 80\\%$$\n",
    "\n",
    "$$ Recall = \\frac{TP}{TP + FN} = \\frac{4}{5} = 80\\%$$\n",
    "\n",
    "$$ F-Score = \\frac{2*Precision*Recall}{Precision + Recall} = 80\\%$$\n",
    "\n",
    "* Overall, we have high precision and recall values on the given set as well, and can't be said to be heavily biased towards any 1 class based on our small dataset.\n",
    "\n",
    "------------------\n",
    "\n",
    "The neural net takes as input the $(R,G,B)$ values for each pixel/unit in image matrix. This gives rise to 3-D matrix input.\n",
    "\n",
    "I have several notions on what abstaction might be at work in the architechture:\n",
    "* Through obtaining realtions and intercombinations of weigths for each pixel's RGB value, the neural net might reach point that nearly mimics the YUV representation of the image (as in VGG-16), where luminosity (Y) and UV components can be participating in a weighted vote.\n",
    "\n",
    "* Neural Nets with complexity near that of VGG-16 are able to abstract out several independent features from images and we analyse them in following points.\n",
    "\n",
    "\n",
    "* As seen from the images, specially the misclassified ones, darker tone and background give the result 'husky' while luminous ones give 'wolf'.\n",
    "\n",
    "\n",
    "* From images 4 and 6, it is clear that it would be very difficult to classify with face features, shape and colour as the main factors. Very subtle features such as shape of ears etc. might be at play, however this implies that the neural network would be amazingly complex, requiring an even bigger training data than initially estimated 5k. This casts further shadow on test set when I think about it.\n",
    "\n",
    "\n",
    "* From image 9, it is clear that colour of the animal alone is not the criteria for the neural net.\n",
    "\n",
    "\n",
    "* From images again, huskies are associated with natural tropical dark backgrounds while wolves are associated with snowy region backgrounds, we can wonder that the training data had bias where background helped in producing a biased estimate. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 7b) \n",
    "Given below are the previously shown images along with its corresponding reconstructed portions obtained from the neural network.\n",
    "\n",
    "The non-gray parts on the reconstructed images are the parts of the image that the neural network thinks are the most important in making the predictions. With the new evidence (assuming you answered previous question without looking into this), please reanswer the above question\n",
    "- 1) How much do you trust the model? Give a subject evlaution of the model\n",
    "- 2) What do you think is the system learning?\n",
    "\n",
    "\n",
    "![Double click and remove the exclamation mark inside the parenthesis to see the image](images/7b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans.\n",
    "I answer the two questions in combined way as follows:\n",
    "\n",
    "* The model blatantly ignores large portions of the image, heeding no significance to facial features, colour of the animal etc. \n",
    "\n",
    "\n",
    "* This type of classification is hard for a human to perform, and to perform classification with accuracy, a human expert would require information on minute features such eyes, ear shape, teeth structure etc. As the neural net simply ignores these, there is no possible way it gives reliable predictions.\n",
    "\n",
    "\n",
    "* As we stated before, it is true that the model is associating luminous backgrounds with wolves. Another interesting thing is that it looks for continuous white patches in image completely free of any black/brown noise to classify as a wolf. Since this completely ignores the animal in question, there are some serious questions about bias in the test-set.\n",
    "\n",
    "\n",
    "* The model, with this perception, is highly likely to perform bad on unseen instances without background cues.\n",
    "\n",
    "\n",
    "* The highest learning determiner as evident from these images seems to be that:\n",
    "    * White patches with eyes/brown colour noise -> husky\n",
    "    * White continous patch -> wolf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment was originally conducted as part of the work. [\"Why Should I Trust You?\"](http://www.arxiv.org/abs/1602.04938): Explaining the Predictions of Any Classifier. \n",
    "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin. In: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining \n",
    "    \n",
    "[Marco](https://homes.cs.washington.edu/~marcotcr/) was kindful enough to share the images used in the experiment"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
